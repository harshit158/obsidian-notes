Measures agreement among raters using measures of reliability. 
There are different such measures that depend on:
- Number of raters
- Whether ratings are:
	- nominal
	- ordinal
	- numerical

---
==Fleiss' Kappa== [Link](https://www.statisticshowto.com/fleiss-kappa/)
- Measures agreement between ==three or more== raters
- Ranges from ==0 to 1==:
	- 0 -> no agreement
	- 1 -> perfect agreement
- ==0.75== is generally considered good
- ==Assumption==: Raters are <u>randomly</u> chosen from population

---
==Cohen's Kappa== [Link](https://www.statisticshowto.com/cohens-kappa-statistic/)
- Only used if
	- two raters each rate one trial
	- one rater rates two trials
- ==Assumption:== Raters are <u>deliberately</u> chosen