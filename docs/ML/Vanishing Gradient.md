![Batch Normalization and ReLU for solving Vanishing Gradients | by Lavanya  Gupta | Analytics Vidhya | Apr, 2021 | Medium|600](https://miro.medium.com/max/2628/0*QbF2QbDvYQF2v3T5.png)

When does it occur:
- Because of saturating ends of sigmoid / tanh
- Problem gets even more magnified when we train deep network, where multiple multiplications of smaller values make the net update very small