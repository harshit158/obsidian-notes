Original META post (https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)
- RAG has two sources of knowledge
	- ==Parametric | Closed book== - (the one that seq2seq models store in their parameters)
	- ==Non-parametric | Open Book== (the one that RAG gets from retrieved docs from a corpus)

RAG VS Fine-tuning
- https://www.rungalileo.io/blog/optimizing-llm-performance-rag-vs-finetune-vs-both
- 