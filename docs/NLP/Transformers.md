==Limitations:==
- Cost of self-attention is $O(N^2)$
Quickly becomes bottleneck for long sequences ^ce649a


![[Pasted image 20220817231907.png]]

![[Pasted image 20230629214640.png]]

http://nlp.seas.harvard.edu/annotated-transformer/

- Example from "Getting started with Google BERT"