==Limitations:==
- Cost of self-attention is $O(N^2)$
Quickly becomes bottleneck for long sequences ^ce649a


![[Pasted image 20220817231907.png]]

![[Pasted image 20230629214640.png]]

- Encoder-only aka ==Auto-encoding models==
- Decoder-only aka ==Auto-regressive models==
- Encoder-Decoder aka ==Sequence-to-sequence models==


http://nlp.seas.harvard.edu/annotated-transformer/

- Example from "Getting started with Google BERT"


## Dimensions

From: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html 
![[Pasted image 20240619181915.png]]