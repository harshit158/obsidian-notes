## Data Formats
![[Pasted image 20220509085651.png|600]]


### Row Major vs Column Major formats


![[Pasted image 20220509101753.png]]
- Famous Paradigms:
	- **CSV** -> Row Major | Consecutive elements in a row are stored next to each other in Memory
		- Better, if want to access features for a particular observation
	- **Parque**t -> Column Major
		- Better, if want to access Timestamps for all the observations

- Row-major formats are better when you have to do a ==lot of writes==, whereas 
  Column-major ones are better when you have to do a ==lot of column-based reads==.

- Pandas Dataframe is in Column Major format -> Access items column wise, not row wise

```python
for col in df.columns:
	for item in df[col]:
		pass
```
is faster than 
```python
for i in range(nrows):
	for item in df.iloc[i]:
		pass
```


### Text Vs Binary format
- Text formats (Eg: CSV) take more space than Binary formats (Eg: Parquet)
- Parquet format preferred by AWS S3, as it is faster to unload and consumers less storage


## Data Models
### Relational Model
- Data is organized into relations (table is just a visual representation of a "relation")
- Relation is a collection of tuples
- Both rows and columns in a relation are unordered (they can be shuffled)

![[Pasted image 20220510080335.png]]

- **Normalization** is a technique to reduce data redundancy by separating contents in different tables. Types - 1NF, 2NF, etc

- SQL: ==Declarative language==:  Specify the outputs you want, and the computer figures out the steps needed to get you the queried outputs
  Python: ==Imperative Language==: Specify the steps needed for an action and the computer executes these steps to return the outputs

- Just like SQL, there can be ==Declarative ML== - specify the task and the feature schema - model building will be taken care of by the framework
	- Eg: **Ludwig** by Uber, **H2O AutoML**



### NoSQL
- Two types of Non relational models:
	- **Document Model**
	  The document model targets use cases where data comes in self-contained documents and relationships between one document and another are rare
	  
	- **Graph Model**
	  Targets use cases where relationships between data items are common and important

- Queries that are made possible in one Data model, is very hard in other data models


### Structured Vs Unstructured
- Structured Data adheres to schema
  Unstructured Data doesn't have to
  
- **Data Lake**: 
	- A repository for storing ==unstructured== data
	- Usually used to store RAW data before processing
	  
  **Data Warehouse**: 
	- For ==structured== data
	- Stores data that has been processed and ready to be used

![[Pasted image 20220510083623.png|600]]


## Data Storage Engines and Processing

- Databases are optimized for:
	- Transactional processing
	- Analytical processing

### Transactional and Analytical Processing
- Transaction can refer to:
	- tweeting
	- booking a ride through apps
	- uploading a new model
	- watching youtube videos

- All these transactions are processed in similar fashion - insert/delete/update records as they are generated. 
  This is known as "==Online Transaction Processing (OLTP)=="

- Transactions are generated by users, hence transactional databases need to be:
	- low latency
	- high availability
	- **Row-major**, since each transaction is independent of others

- Row-major is not apt for analytical processing, hence we need data **Analytical Databases** that stores data in Column-major format.
  This is known as "==Online Analytical Processing (OLAP)=="

- But now, both types of databases can handle both of types of processing.
  They do it by **decoupling storage from processing**. 
  Eg: Bigquery, Snowflake, Teradata   


## Modes of Data Flow
- 3 Main modes of dataflow across different processes:
	- (1) **Through Databases**
		- Cons:
			(1)  Both processes need to access the same database, not possible always
			(2) Slow I/O operations from DBs can be latency heavy
			
	- (2) **Through Services using Requests** (REST APIs)
		- Most popular style of requests:
			(1) REST
			(2) RPC (Remote Procedure Call)
			
	- (3) **Through Real Time Transport** (Apache Kafka, Amazon Kinesis)


ðŸ¥Š  **Through Real Time Transport**

- Data flow in real time can happen through 2 architectures:
	(1) ==Request Driven== Architecture
	(2) ==Even Driven== Architecture


ðŸ¥Š  **==Request Driven== Architecture**

![[Pasted image 20220512092423.png|600]]
- This architecture can blow up if a single service stops working 
- Instead of this, there can be a broker that takes care of passing data to whoever wants it


ðŸ¥Š  **==Event Driven== Architecture**

![[Pasted image 20220512092809.png|600]]

- Instead of requesting data from the service, the service **broadcasts** its results to a broker which makes the results available to whosoever wants it.

- DB can be broker, but it can be slow because of frequent I/O operations
  Instead of DB, **Real time transport** makes use of **in-memory storage** as broker
  
- A piece of data broadcast to real time transport is called an ==Event==. Hence "Event Driven" architecture.
  A real time transport is also called ==Event Bus==

- Request-driven architecture works well for systems that rely more on <u>logic</u> than on data Event-driven architecture works better for systems that are <u>data-heavy</u>.

- 2 Types of Real Time Transports:
	- (1) **Pubsub (publish-subscribe)**
		- Any service can publish to real time transport
		- Any service that subscribes to a topic can read all events in that topic
		- Services that produce data, don't care about who consumes their data
		- Eg: Apache Kafka, Amazon Kinesis
		  ![[Pasted image 20220512101708.png]]
		  
	- (2) **Message Queue**
		- Even often has intended consumers (an event with intended consumers is called ==message==)
		- Message queue is responsible to deliver message to right consumers
		- Eg: Apache RocketMQ, RabbitMQ
		  ![[Pasted image 20220512101726.png]]

## Batch Processing VS Stream processing

- Tools for efficient stream processing:
	- Apache Flink
	- KSQL
	- Spark Streaming